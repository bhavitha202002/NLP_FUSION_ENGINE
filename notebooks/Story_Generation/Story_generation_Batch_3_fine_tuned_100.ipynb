{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3zoyFMqHtnH",
        "outputId": "c393f589-7e1f-4b0c-86df-0062ac3277c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# First upload the training and evaluation files to this runtime (Press connect if needed)\n",
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ihb_jZIqH1ee"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    AutoConfig,\n",
        "    GPT2LMHeadModel,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    LineByLineTextDataset,\n",
        "    PreTrainedTokenizer,\n",
        "    TextDataset,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "# Setup logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Get access to model types and model configs to select GPT2 model and config\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0n2hvXNiH_sb"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\"\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"If training from scratch, pass a model type from the list: \"\n",
        "            + \", \".join(MODEL_TYPES)\n",
        "        },\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"Where do you want to store the pretrained models downloaded from s3\"\n",
        "        },\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3Hb6lB1jIAhp"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    train_data_file: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n",
        "    )\n",
        "    eval_data_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"\n",
        "        },\n",
        "    )\n",
        "    line_by_line: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    mlm: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Train with masked-language modeling loss instead of language modeling.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    block_size: int = field(\n",
        "        default=-1,\n",
        "        metadata={\n",
        "            \"help\": \"Optional input sequence length after tokenization.\"\n",
        "            \"The training dataset will be truncated in block of this size for training.\"\n",
        "            \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"},\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J5uDK7GCIEeL"
      },
      "outputs": [],
      "source": [
        "# Create LineByLineDataset from Movie Plots text file\n",
        "def get_dataset(\n",
        "    args: DataTrainingArguments, tokenizer: PreTrainedTokenizer, evaluate=False\n",
        "):\n",
        "    file_path = args.eval_data_file if evaluate else args.train_data_file\n",
        "    if args.line_by_line:\n",
        "        return LineByLineTextDataset(\n",
        "            tokenizer=tokenizer, file_path=file_path, block_size=args.block_size\n",
        "        )\n",
        "    else:\n",
        "        return TextDataset(\n",
        "            tokenizer=tokenizer,\n",
        "            file_path=file_path,\n",
        "            block_size=args.block_size,\n",
        "            overwrite_cache=args.overwrite_cache,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7YoYghZZIGnC"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    model_args = ModelArguments(\n",
        "        model_name_or_path=\"gpt2\", model_type=\"gpt2\"\n",
        "    )\n",
        "    data_args = DataTrainingArguments(\n",
        "        train_data_file=\"6_genre_clean_training_data.txt\",\n",
        "        eval_data_file=\"6_genre_eval_data.txt\",\n",
        "        line_by_line=True,\n",
        "        block_size=512,\n",
        "        overwrite_cache=True,\n",
        "    )\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"story_generator_checkpoint\",\n",
        "        overwrite_output_dir=True,\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "\n",
        "        logging_steps=500,\n",
        "        per_device_train_batch_size=4,\n",
        "        num_train_epochs=3,\n",
        "        save_total_limit=1,\n",
        "        save_steps=1000,\n",
        "    )\n",
        "\n",
        "    if data_args.eval_data_file is None and training_args.do_eval:\n",
        "        raise ValueError(\n",
        "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "            \"or remove the --do_eval argument.\"\n",
        "        )\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        training_args.local_rank,\n",
        "        training_args.device,\n",
        "        training_args.n_gpu,\n",
        "        bool(training_args.local_rank != -1),\n",
        "        training_args.fp16,\n",
        "    )\n",
        "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
        "\n",
        "    # Set seed for deterministic training runs\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.model_name_or_path, cache_dir=model_args.cache_dir\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path, cache_dir=model_args.cache_dir\n",
        "    )\n",
        "\n",
        "    model = GPT2LMHeadModel.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "        config=config,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "\n",
        "    special_tokens_dict = {\n",
        "        \"bos_token\": \"<BOS>\",\n",
        "        \"eos_token\": \"<EOS>\",\n",
        "        \"pad_token\": \"<PAD>\",\n",
        "        \"additional_special_tokens\": [\n",
        "            \"<superhero>\",\n",
        "            \"<action>\",\n",
        "            \"<drama>\",\n",
        "            \"<thriller>\",\n",
        "            \"<horror>\",\n",
        "            \"<sci_fi>\",\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if data_args.block_size <= 0:\n",
        "      # If block_size <= 0, set it to max. possible value allowed by model\n",
        "        data_args.block_size = model.config.max_position_embeddings\n",
        "        #data_args.block_size = tokenizer.max_len\n",
        "    else:\n",
        "        #data_args.block_size = min(data_args.block_size, tokenizer.max_len)\n",
        "        data_args.block_size = min(data_args.block_size, model.config.max_position_embeddings)\n",
        "\n",
        "\n",
        "    # Get datasets\n",
        "\n",
        "    train_dataset = (\n",
        "        get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\n",
        "    )\n",
        "    eval_dataset = (\n",
        "        get_dataset(data_args, tokenizer=tokenizer, evaluate=True)\n",
        "        if training_args.do_eval\n",
        "        else None\n",
        "    )\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=data_args.mlm,\n",
        "    )\n",
        "\n",
        "    # Initialize our Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=None,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    try:\n",
        "      if training_args.do_train:\n",
        "          model_path = (\n",
        "              model_args.model_name_or_path\n",
        "              if model_args.model_name_or_path is not None\n",
        "              and os.path.isdir(model_args.model_name_or_path)\n",
        "              else None\n",
        "          )\n",
        "          trainer.train(model_path=model_path)\n",
        "          trainer.save_model()\n",
        "          tokenizer.save_pretrained(training_args.output_dir)\n",
        "    except KeyboardInterrupt:\n",
        "      print(\"Saving model that was in the middle of training\")\n",
        "      trainer.save_model()\n",
        "      tokenizer.save_pretrained(training_args.output_dir)\n",
        "      return\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if training_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "        eval_output = trainer.evaluate()\n",
        "\n",
        "        perplexity = math.exp(eval_output[\"eval_loss\"])\n",
        "        result = {\"perplexity\": perplexity}\n",
        "\n",
        "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n",
        "        '''if trainer.is_world_master():\n",
        "            with open(output_eval_file, \"w\") as writer:\n",
        "                logger.info(\"***** Eval results *****\")\n",
        "                for key in sorted(result.keys()):\n",
        "                    logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "                    writer.write(\"%s = %s\\n\" % (key, str(result[key])))'''\n",
        "        if trainer.args.local_rank in [-1, 0]:\n",
        "          with open(output_eval_file, \"w\") as writer:\n",
        "            logger.info(\"***** Eval results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "              logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "              writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "\n",
        "        results.update(result)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZHQOAHixIIsP"
      },
      "outputs": [],
      "source": [
        "#!pip uninstall transformers accelerate\n",
        "#!pip install transformers[torch]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "xr92C6HRIKxi",
        "outputId": "37e84105-6604-4e09-f09d-f228c53fe2b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1482: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='782' max='1041' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 782/1041 07:36 < 02:31, 1.71 it/s, Epoch 2.25/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>4.097400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1041' max='1041' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1041/1041 10:18, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>4.097400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.278000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:49]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Press the Run Cell button to the left to start training\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# To stop training and save model, press the same Run Cell button (now, it is the Interrupt Execution button)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JBQnYI4SIQ1f"
      },
      "outputs": [],
      "source": [
        "# This cell is to style the Google Colab's output properly (Just blindly run this)\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "y35UoRUoIRn9",
        "outputId": "d8d4c33c-7ba9-4318-fa55-c2d875e87c64"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# Run these cells for story generation\n",
        "from transformers import pipeline, TextGenerationPipeline, GPT2LMHeadModel, AutoTokenizer\n",
        "\n",
        "\"\"\"\n",
        "Below, my model checkpoint is commented out. You can replace your checkpoint\n",
        "with that to test story generation if your checkpoint didn't train for long enough\n",
        "\"\"\"\n",
        "#checkpoint = \"pranavpsv/gpt2-genre-story-generator\"\n",
        "checkpoint = \"story_generator_checkpoint\"\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "story_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
        "# The format for input_prompt: \"<BOS> <genre> Optional text...\"\n",
        "# Supported genres: superhero, sci_fi, horror, thriller, action, drama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "PXWTEnbjIUOf",
        "outputId": "cb814de2-315c-4c0b-9c5e-5b685a272963"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Story : \n",
            " [{'generated_text': \"<BOS> <horror> On a cold winter's evening in February, three strangers are brutally murdered on the street. Soon after their arrival at an abandoned school for gifted children who was once part of another notorious cult; two friends — Drs Nels and Pucket—come upon this as if it has been some kind family reunion event rather than just random shooting deaths that can't be fixed by any sane man (but maybe by Mrs Dummett). One student starts talking to himself about how he did\"}]\n"
          ]
        }
      ],
      "source": [
        "input_prompt = \"<BOS> <horror>\"\n",
        "story = story_generator(input_prompt, max_length=100, do_sample=True,\n",
        "               repetition_penalty=1.1, temperature=1.2,\n",
        "               top_p=0.95, top_k=50)\n",
        "print(\"\\n Story : \\n\",story)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "R3hSvKUpkb3A",
        "outputId": "0a1e522e-f4cd-418d-db14-254c39d76361"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Story : \n",
            " [{'generated_text': '<BOS> <action> In 1960, at their graduation ceremony in Latham County—an affluent rural area of Louisiana with an attractive population as well (including the recently revealed owner), who has run a dairy farm along Big River Highway near New Orleans to raise cattle. As part way from home and not entirely within sight that day on his morning walk across large landings they find what appear like four beautiful women riding horseback over riverbeds into town: Mary Louise McCollum; Mrs., Missy Jones'}]\n"
          ]
        }
      ],
      "source": [
        "input_prompt = \"<BOS> <action>\"\n",
        "story = story_generator(input_prompt, max_length=100, do_sample=True,\n",
        "               repetition_penalty=1.1, temperature=1.2,\n",
        "               top_p=0.95, top_k=50)\n",
        "print(\"\\n Story : \\n\",story)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "kH11BnDlLTOw",
        "outputId": "7d282452-ff85-481f-e451-6d6a94c82706"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Story : \n",
            " [{'generated_text': \"<BOS> <drama> In 1942, a newly minted Nazi doctor named Rudolf (Boruch) is trying to help an orphan murdered by German troops after his wife has been raped. In the meantime though there are rumors that Dr Bessie (Ruth Westhoff-Olney), her superior's mistress and also fellow physician who had performed similar services for the wounded widowed woman of death in her home town near Berlin on several occasions when she was pregnant then turned herself over earlier than usual because\"}]\n"
          ]
        }
      ],
      "source": [
        "input_prompt = \"<BOS> <drama>\"\n",
        "story = story_generator(input_prompt, max_length=100, do_sample=True,\n",
        "               repetition_penalty=1.1, temperature=1.2,\n",
        "               top_p=0.95, top_k=50)\n",
        "print(\"\\n Story : \\n\",story)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "AmI_1L1SkslI",
        "outputId": "97ec299a-ab69-4dd9-ee74-076fc6b44b30"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Story : \n",
            " [{'generated_text': \"<BOS> <sci_fi> One day in 1970, as his family are preparing to retire before the election night. Joe arrives at a hotel suite and begins thinking about becoming CEO if he succeeds Mr Nixon or not; instead of going forward with another term until there's money running around by him then having nothing more than five seconds after hiring someone otherworldly for such reasons I'm afraid some people will have trouble believing they're in it so good while others try something different! After one person makes out her voice that way\"}]\n"
          ]
        }
      ],
      "source": [
        "input_prompt = \"<BOS> <sci_fi>\"\n",
        "story = story_generator(input_prompt, max_length=100, do_sample=True,\n",
        "               repetition_penalty=1.1, temperature=1.2,\n",
        "               top_p=0.95, top_k=50)\n",
        "print(\"\\n Story : \\n\",story)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "bSnU4vp7lGYo",
        "outputId": "48753146-cb89-4e7d-b1ee-dca8ac46dbcd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Story : \n",
            " [{'generated_text': '<BOS> <thriller> The story unfolds as both protagonists are faced with the death of their childhood sweetheart, whom they do not know. They must figure out a way to stop this from happening again. While trying in vain for love he learns that some girl has died and was left completely blind by her blindness - leading them back into despair once more along paths they did choose never come across before (in time), culminating at one such night when their car breaks down. After leaving his office on an expressway'}]\n"
          ]
        }
      ],
      "source": [
        "input_prompt = \"<BOS> <thriller>\"\n",
        "story = story_generator(input_prompt, max_length=100, do_sample=True,\n",
        "               repetition_penalty=1.1, temperature=1.2,\n",
        "               top_p=0.95, top_k=50)\n",
        "print(\"\\n Story : \\n\",story)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "p7AQEzlZlUg4",
        "outputId": "6f85e11d-2086-4dcb-83e7-336e86c58591"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "9aTRHT-IQ4TR",
        "outputId": "3a376984-09ac-41fb-94f1-b396e80e660d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/ColabNotebook/Story_generation_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/ColabNotebook/Story_generation_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/ColabNotebook/Story_generation_model/vocab.json',\n",
              " '/content/drive/MyDrive/ColabNotebook/Story_generation_model/merges.txt',\n",
              " '/content/drive/MyDrive/ColabNotebook/Story_generation_model/added_tokens.json',\n",
              " '/content/drive/MyDrive/ColabNotebook/Story_generation_model/tokenizer.json')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the final model after training and evaluation to Google Drive\n",
        "model.save_pretrained(\"/content/drive/MyDrive/ColabNotebook/Story_generation_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/ColabNotebook/Story_generation_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y3cQ6bdmM1G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
